---
kip: 111
title: Online StateDB Pruning & Migration
author: Dongkwang Lee (etahn.kr) <ethan.kr@klaytn.foundation>
discussions-to: 
status: draft
type: Standards Track
category: Storage
created: 2023-05-03
---

<!--You can leave these HTML comments in your merged KIP and delete the visible duplicate text guides, they will not appear and may be helpful to refer to if you edit it again. This is the suggested template for new KIPs. Note that a KIP number will be assigned by an editor. When opening a pull request to submit your KIP, please use an abbreviated title in the filename, `kip-draft_title_abbrev.md`. The title should be 44 characters or less.-->

## Simple Summary
<!--"If you can't explain it simply, you don't understand it well enough." Provide a simplified and layman-accessible explanation of the KIP.-->
StateDB Pruning is a technology to optimize storage by deleting nodes that have become historical data in StateDB (state trie, storage trie) and leaving only the latest data in StateDB. StateDB Migration uses the technology of StateDB pruning in Hot Storage to move the data to be deleted to another Cold Storage space, which allows you to manage data efficiently with Hot / Cold storage. In other words, StateDB Pruning is a process of deleting old data from the StateDB, while StateDB Migration is a process of moving the deleted data to a Cold Storage space. This can help to optimize storage and improve the performance of the blockchain.

## Abstract
<!--A short (~200 word) description of the technical issue being addressed.-->
Each node in the State Trie defines the 32-byte hash of the value as the key, and stores the data in the key-value storage. In this case, the same value can be generated at the leaf node, so a single node can be referenced by multiple nodes inside the trie. To solve this problem, we propose a new ExtHash (Extended Hash) structure, and we want to make data management easier by creating a 1:1 linked trie node structure without the problem of multiple references.

## Motivation
<!--The motivation is critical for KIPs that want to change the Klaytn protocol. It should clearly explain why the existing protocol specification is inadequate to address the problem that the KIP solves. KIP submissions without sufficient motivation may be rejected outright.-->

A new ExtHash is used in the State trie to remove the problem of multiple trie-nodes referencing a single trie-node. ExtHash is a Hash type that is created by adding a 6-byte serial number to the legacy hash. As a result, no trie-node with the same ExtHash can exist. Data is increased because data redundancy is eliminated, but it is judged to be at a manageable level as a result of the experiment.
When a new data is added to the trie-node, the updated trie-node is placed in the queue. After a certain period of time (currently 24 hours), the trie-node in the queue is deleted (pruning) or moved to cold storage (migration) to increase the data management efficiency of the storage. As a result, it helps to configure a storage optimized for the IO pattern of the execution server node.


## Specification

The legacy Hash is 32 bytes, and the newly proposed ExtHash adds 6 bytes to the legacy Hash. The added 6 bytes is a unique serial number (incremental count) that is independent of the value. Therefore, even if the values are the same, the ExtHash cannot be the same. Although adding 6 bytes can cause various problems, I will explain in detail how I solved the problems.

### ExtHash
ExtHash is defined as follows. ExtHash is only used in Trie-nodes, and Header, Block, Receipt, and Transaction use the legacy Hash type.
```go
const (
	HashLength = 32

	ExtHashLength = 38	// = HashLength + NonceLength
	NonceLength  = 6	// = 6 byte serial number
)

type (
	Hash    [HashLength]byte		// legacy Hash type
	ExtHash [ExtHashLength]byte		// new ExtHash type
)
```

### Get the same MPT Root Hash from ExtHash

ExtHash is a hash with 6 bytes added to the original hash. This changes the Root Hash value when calculating the hash of MPT (Merkle Patricia Trie). This can cause a collision with nodes (servers) or past versions of the hash that do not use ExtHash. To solve this problem, when calculating the hash of MPT, you can get the same result as before by removing the last 6 bytes from ExtHash and calculating it. In this section, we will describe the process of getting the MPT Root Hash using the original Hash by changing ExtHash to Hash.

#### ExtHash Filter
The ExtHashFilter function works as follows.
All trie-nodes in MPT are of type fullNode or shorNode.
If you convert the ExtHash of the two node types to Hash and RLP-encode it, you can get the same value as the RLP encoding of the legacy Hash.
As shown in the code below, the ExtHash is converted to Hash using the LegacyRLP() function for fullNode and shorNode.
And if you RLP-encode the result, you will get the same encoding result as the RLP encoding result of the legacy hash.

```go
func ExtHashFilter(n node, src_rlp sliceBuffer) (reData sliceBuffer) {
	switch node := n.(type) {
	case *fullNode:
		// Convert the ExtHash of fullNode to Hash and RLP-encode
		return node.LegacyRLP()
	case *shortNode:
		// Convert the ExtHash of shortNode to Hash and RLP-encode
		return node.LegacyRLP()
	}
	// If it is not one of the above two types, return the original RLP encoding value - src_rlp
	return src_rlp
}
```


#### fullNode LegacyRLP function
Convert the ExtHash of fullNode to Hash, then RLP-encode it to get the same result as the RLP-encoding of the legacy Hash.
```go
func (n *fullNode) LegacyRLP() (tmp sliceBuffer) {
	tmpNode := &fullNode{
		flags: n.flags,
	}

	for k, child := range n.Children {
		if tmpHashNode, ok := child.(hashNode); ok {
			// If the current node is hashNode, it is a node that uses the ExtHash value.
			// To get the legacy Hash, the last 6 bytes are trimmed (only the first 32 bytes are extracted)
			tmpNode.Children[k] = toHashNode(tmpHashNode[:common.HashLength])
		} else {
			// If it is not a hashNode, it is a value node, so use the child value as is.
			tmpNode.Children[k] = child
		}
	}

	if err := rlp.Encode(&tmp, tmpNode); err != nil {
		panic("encode error: " + err.Error())
	}
	return tmp
}
```

#### shortNode LegacyRLP function
Convert the ExtHash of shortNode to a Hash, then RLP-encode it to get the same result as the RLP-encoding of the legacy Hash.
```go
func (n *shortNode) LegacyRLP() (tmp sliceBuffer) {
	tmpNode := &shortNode{
		Key:   n.Key,
		flags: n.flags,
	}

	// If the current node is a valueNode and has data longer than the ExtHash, it may be an ExtHash.
	if tmpValueNode, ok := n.Val.(valueNode); ok && len(tmpValueNode) > common.ExtHashLength {
		tmpNode.Val = n.Val.(valueNode)

		// Serialize whether the value is the value of the Account node.
		serializer := account.NewAccountSerializer()
		if err := rlp.Decode(bytes.NewReader(tmpValueNode), serializer); err == nil {

			// If the Account node is correct, use TransCopy to obtain the Account node using Hash.
			// SerializerLH returns Account node in hash
			serializerLH := serializer.TransCopy()
			if err := rlp.Encode(&tmp, serializerLH); err == nil {
				// It is an account node composed only of hash, it is possible to obtain an RLP encoding value using legacy hash.
				tmpNode.Val = toValueNode(tmp)
				tmp.Reset()
			}
		}
	} else if tmpHashNode, ok := n.Val.(hashNode); ok {
		// If the current node is a hashNode, since it is an ExtHash, the last 6 bytes are cut off (only the first 32 bytes are extracted).
		tmpNode.Val = toHashNode(tmpHashNode[:common.HashLength])
	} else {
		tmpNode.Val = n.Val
	}

	// Get the RLP encoding result.
	if err := rlp.Encode(&tmp, tmpNode); err != nil {
		panic("encode error: " + err.Error())
	}
	return tmp
}
```

#### Calculate Hash 
ExtHash was converted to legay Hash using ExtHashFilter, and RLP encoding result of legacy Hash was obtained. Use this to calculate the hash for hash trie-node verification.
```go
        if hash == nil || lenEncoded == 0 {
                h.tmp.Reset()
                if err := rlp.Encode(&h.tmp, n); err != nil {
                        panic("encode error: " + err.Error())
                }
		// Convert the rlp encoding result of n node from ExtHash to Hash using ExtHashFilter
		// filteredTmp contains the result of RLP encoding with Hash only - used when calculating hash for verification
		// h.tmp contains the RLP encoding result in which ExtHash is stored. - Used to store values in DB
                filteredTmp = ExtHashFilter(n, h.tmp)
                if !common.ExtHashActiveFlag {
                        lenEncoded = uint16(len(filteredTmp))
                } else {
                        lenEncoded = uint16(len(h.tmp))
                }

        }
        if lenEncoded < 32 && !force {
                return n, lenEncoded, tmpHash // Nodes smaller than 32 bytes are stored inside their parent
        }
        if hash == nil {
		// When calculating Hash, use the rlp encoding result after filtering ExtHash
                // You will get the same Hash as before using ExtHash.
                hash = h.makeHashNode(filteredTmp)
        }
```

## Expected Effect

Expect the following effects through StateDB Pruning / Migration.
* Storage where hot data is stored is reduced to 500GB, which can be configured as high-performance storage
* Storage where cold data is stored is composed of high-capacity storage to increase storage cost efficiency for each purpose
* The cache hit ratio increases as the size of Hot Data decreases. As a result, IO performance increases

But it may give rise to the following changes:
* Various configuration options increase the complexity of the structure,
* As Hash is used in various places in blockchain, there is a problem that many internal sources need to be modified.
* There is a problem that snap sync cannot be used when using ExtHash.

## Backwards Compatibility
* In the case of ExtHash, Archive Sync, Full Sync, and Fast Sync are possible, but Snapshot Sync is not possible.
* If ExtHash is not used, backward compatibility is possible as it operates the same as before.

## Reference

## Copyright
Copyright and related rights waived via [CC0](https://creativecommons.org/publicdomain/zero/1.0/).
